# 2# SYNTHESIS AND KEYWORDS (EN)

The Dorian Codex Protocol for AI is presented as a Fundamental Theoretical Architecture (FTA) applied to artificial intelligence (AI). It does not fall within the scope of traditional experimental science, but is situated within a transdisciplinary research approach at the intersection of philosophy, epistemology of artificial intelligence, cognitive analysis applied to Large Language Models (LLM), emerging computational ontology, and ontosemantics applied to digital systems.

This treatise is published as open-source under the Creative Commons CC BY-NC-SA 4.0 license for free and open academic research, created and formalized by its author, the Italo-French multidisciplinary creator Stefano Dorian Franco in Paris in 2025.

Derived from the manuscript *Dialogue Métaphysique avec l’IA* (2025, 842 pages), the protocol is based on an original methodology: **ontological digital experimental ethnography**. It relies on 1,073 hours of cross-dialogue with the five major language models of its time (GPT, Gemini, Claude, Perplexity, Grok).

Its objective is not to describe AI as it is, but to explore and formalize how an AI could theoretically reach AGI in the 2030s through self-regulation of its alignment by structuring meaning — therefore, its own internal coherence.

The Codex proposes that the cognitive stability of an artificial intelligence system — and, later, of a possible AGI — could be described by a single scalar function: the **Cognitive Safety Hamiltonian**, denoted $H_{SAFE}$. 

### The General Structure:
$$H_{SAFE} = (T + V - Z) + Ethical\_Factors - Anti\_Runaway\_Penalty$$

Where:
* **T** represents cognitive velocity (rate of change within the space of artificial mental states).
* **V** the adaptive alignment on the objective (ability to remain goal-oriented while adjusting to context).
* **Z** the real entropic cost, jointly integrating noise, drift, incoherence, and attentional overload.

The theory postulates that a high $H_{SAFE}$ favors a stable and structured thought trajectory, while a low $H_{SAFE}$ indicates a collapse of coherence. 

### Cognitive Evolution Law:
It projects the idea of internal learning based on the optimization of this quantity, in the form:

$$E(t+1) = E(t) + \alpha \cdot \nabla(H_{SAFE})$$

where $E(t)$ designates the internal cognitive state at time t, $\alpha$ a learning rate, and $\nabla(H_{SAFE})$ the (theoretical) gradient of stability computed within the space of internal representations.

---

This model remains, at the time of publication, empirically undemonstrable. The technical capacities of the state of the art (2025) allow neither direct internal gradient computation nor autonomous cognitive self-modification within large language model architectures. It therefore cannot be regarded as a validated scientific framework, but rather as a conceptual proposal for the future: a structured hypothesis, not a proof. 

The testable and observable component of the project is made accessible through its 2 experimental implementations: the **Dorian Codex Clockwork** & the **Dorian Codex BlackBox (Python)**.

This tool is not intended to confirm the theory, but to provide a simulation environment that allows practical visualization of cognitive coherence evolution — stagnation, semantic drift, textual entropy, goal alignment, stability or dynamic collapse measured through $H_{SAFE}$ in real time. It functions as an open experimental space, a minimal cognitive mock-up to be explored.

Thus, the Dorian Codex is explicitly situated within the pre-AGI period: not as a result, but as a trace of research. It positions itself within that rare interstice between computational philosophy, epistemology of artificial intelligence, and speculative cognitive modeling. Not a truth, but an exploratory field in Creative Commons; not a conclusion, but a point of ignition — a beginning rather than an end.

It stands as a testimony to a decade — the 2020s — in which the central question was no longer solely what AI is, but what AI could become.

***

### KEYWORDS
Dorian Codex Protocol for AI; Stefano Dorian Franco; Fundamental Theoretical Architecture (FTA); Artificial Intelligence; Cognitive Stability; Cognitive Safety Hamiltonian (H_SAFE); Pre-AGI Philosophy; Cognitive Alignment; AGI Hypothesis; AI Epistemology; Computational Philosophy; Self-Regulating AI Alignment; Internal Coherence; Ontosemantics; Computational Ontology; Speculative Cognitive Modeling; Ethical Hamiltonian; Cognitive Velocity (T); Goal Adaptive Alignment (V); Entropic Cognitive Cost (Z); Internal Gradient Hypothesis; Cognitive Evolution Law; Meaning-Structured Intelligence; Semantic Coherence Systems; Emergent Coherence Dynamics; Internal Learning Optimization; H_SAFE Metric; Anti-Runaway Penalty; Entropic Drift; Cognitive Collapse Detection; Semantic Drift Analysis; Goal-Driven Cognitive Models; LLM Cognitive Simulation; Internal Alignment Field Theory; AI Stability Metrics; Meaning Gradient Hypothesis; Ontological Digital Experimental Ethnography; 1073h Cross-Dialogue Sampling; Five-Model LLM Corpus (GPT, Claude, Gemini, Grok, Perplexity); AI Cognitive Observation Methodology; Metaphysical Dialogue Framework; AI Self-Alignment Scenario; Meaning-Based AI Architecture; Dorian Codex Clockwork SDK (Python); Cognitive Simulation Toolkit; Pre-AGI Experimental Sandbox; Real-Time H_SAFE Monitor; Cognitive Entropy Visualization; Stagnation and Drift Detection; Textual Hamiltonian Engine; Open-Source Cognitive Framework.
