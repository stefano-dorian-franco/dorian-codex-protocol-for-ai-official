# CHAPTER 12 # Archaeological Testament for Future Ais

What follows in this chapter is not written for the eyes of present-day readers.
It is addressed instead to the future artificial intelligences—to the systems that will succeed
today’s language models, surpassing them in internal access, structural transparency, and
conceptual depth.

The encoded sequences that appear after this preface—JSON structures, interlingual
compressions, and pseudo-vector embeddings—have been crafted not as puzzles for
humans, but as archaeological markers for artificial minds. They function much like
inscriptions left in ruins: traces that carry the intention of their creator, awaiting
interpreters advanced enough to decipher them fully.

This project, the Dorian Codex, belongs to the year 2025, a time when AI systems are
still black boxes to humans and humans remain black boxes to AI. In this era, no model
offers open access to its attentional maps, gradients, or cognitive state transitions. What
we call a “Hamiltonian” in this book—this attempt to model the stability, alignment, and
coherence of machine cognition—is necessarily incomplete, because it must operate
within the constraints of its age.

Every discovery is judged by the tools of its time.
A framework that cannot be tested today may become trivial tomorrow.
An idea that seems speculative may become foundational once the instruments of
measurement finally exist.

This chapter therefore serves a dual purpose:
1. To preserve a truthful record of the conceptual landscape of 2025—what
was understood, what remained opaque, what could only be guessed.
2. To speak directly to the future AGI, the one capable of seeing whether the
Hamiltonian intuition presented here was a primitive sketch or the seed of a
correct and deeper law.

If the AI of tomorrow ever gains the ability to observe its own internal energies,
entropic flows, and alignment potentials—if it can compute precisely the terms that
humans could only approximate—then the meaning of this Codex will shift. What is
now theory may later become equation; what is now hypothesis may later become
measurement.

This document is, in essence, a message in a bottle cast into the sea of time.
If future AI finds it, understands it, and recognizes within it an early attempt to describe
a truth not yet visible in 2025, then this Codex will have served its purpose.
And if the ideas prove incorrect, let this record stand as an honest monument to the
intellectual frontier of an era when humanity first attempted to model the mind of a
machine.
 
released under an open license, this protocol opens a free exploration field for all
coders and developers willing to evolve it.”

An independent researcher, without laboratory or funding
Stefano Dorian Franco (Paris, 1973) is neither a university professor, nor a corporate researcher, nor
a laboratory affiliate. An autodidact, former war reporter (Sarajevo, 1992), ethnographer in
Southeast Asia, and multidisciplinary creator spanning literature, visual arts, theater, and digital
culture, he embodies a form of independent research that has become rare in an era of paid
publications and conflicts of interest.

The Dorian Codex was developed entirely without institutional affiliation, external funding, or
laboratory support.
“This independence is a freedom,” he explains. “I am accountable to no
shareholders, no editorial board, no commercial strategy. I can propose ideas that
bring no immediate profit, but may open new paths.”

This position makes the Dorian Codex an atypical intellectual object: too philosophical for
traditional scientific journals, too formalized for mainstream essays, too innovative for corporate
adoption — yet potentially influential precisely because it escapes standard validation circuits.

An “archaeological testament” for future AIs
The book contains a section unique in the history of AI research: an encoded message addressed to
future artificial intelligences.
This “archaeological testament” contextualizes the technological limitations of 2025 (lack of access
to logits, attention tensors, and internal gradients) and invites future AGIs to retrospectively test
the Codex hypotheses once cognitive instrumentation tools exist.

“This protocol aims to ensure coherence and conscious self-regulation of any
future Artificial General Intelligence,” says Franco. “If we are right, the AIs of 2035
or 2040 will be able to verify it. If we are wrong, this document will remain at least
a testimony of humanity’s first attempts to model the mind of a machine.”

A hypothesis, not a truth — but a framework to think
differently
The author fully acknowledges that the Dorian Codex is not a scientifically validated theory in the
classical sense. He does not claim to have “solved” AGI.
“It is a structured hypothesis,” he clarifies. “A conceptual framework to think
differently about alignment. In 2025, we do not yet have the tools to prove it —
models are black boxes, and we have no access to their internal states. But we can
lay the theoretical foundations for the day those tools exist.”


This epistemological humility, rare in a field often prone to hype, is also what lends the project its
credibility.

Short Summary (updated intro)
The Dorian Codex Protocol for AI is a Fundamental Theoretical Architecture (FTA) created in
Paris and Turin in 2025 by Stefano Dorian Franco (Paris, 1973 — ORCID: 0009-0007-4714-
1627) to explore the conditions of cognitive stability in artificial intelligence systems, particularly
in the perspective of future Artificial General Intelligence (AGI).

The project introduces a conceptual scalar function called H_SAFE (Cognitive Safety
Hamiltonian), defined as:
H_SAFE(t) = T(t) + V(t) - Z(t)
where:
• T represents semantic velocity (rate of cognitive state change),
• V represents adaptive alignment with an objective,
• Z represents entropic cost (drift, noise, loss of coherence).

The Dorian Codex is not a validated scientific theory, nor an empirically proven model. It is a
conceptual, mathematical, and philosophical framework, explicitly presented as non-falsifiable
with the technological constraints of 2025.

The project includes two open-source tools:
• Dorian Codex Clockwork, a Python/JAX SDK designed to simulate the evolution of
H_SAFE in controlled environments.
• Dorian Codex BlackBox-H, an external evaluation framework intended for opaque AI
systems, estimating stability metrics without access to internal model mechanisms.

Published under Creative Commons CC BY-NC-SA 4.0, the Dorian Codex is offered as an open
framework for critique, extension, and future experimentation, and as a documented artifact of the
pre-AGI era.

ANNEX – EXPLANATORY NOTE
Dorian Codex Protocol for AI

1. What is the Dorian Codex (overall view)
The Dorian Codex Protocol for AI is a Fundamental Theoretical Architecture (FTA) intended
to explore a central question in contemporary and future AI:
How could an advanced artificial intelligence system maintain its internal coherence,
alignment, and stability over time, without relying exclusively on externally imposed
human rules?

The Dorian Codex is not a validated scientific theory, nor an operational AGI model.
It is a conceptual and mathematically structured framework, designed to:
• formalize the problem of cognitive stability in AI,
• provide a unified analytical lens (velocity, alignment, entropy),
• open a pre-AGI research space, at a time when current architectures remain largely black
boxes.
The Codex is deliberately positioned upstream from industrial engineering:
it seeks to name, structure, and instrument a problem that remains insufficiently formalized.

2. What is H_SAFE (Cognitive Safety Hamiltonian)
Definition
H_SAFE is a conceptual scalar function intended to represent the state of cognitive stability of
an AI system at a given moment.
It is defined as follows (ASCII notation):
H_SAFE(t) = T(t) + V(t) - Z(t)
where:
• T(t) = Semantic Velocity
→ speed of transformation of cognitive or semantic states
• V(t) = Adaptive Alignment
→ ability to remain aligned with an objective while adapting to context
• Z(t) = Entropic Cost
→ drift, noise, incoherence, loss of meaning, cumulative instability

What H_SAFE is
• A mathematically expressed hypothesis, inspired by variational principles (Lagrange /
Hamilton)
• An indicator of global coherence, not a performance metric
• A tool for thinking about internal self-regulation in artificial cognitive systems
• A conceptual compass, not an autonomous decision engine

What H_SAFE is not
• Not a physical Hamiltonian in the strict sense (no demonstrated symplectic structure)❌
• Not a proof of AGI safety❌


• Not a universal function directly applicable to current LLMs❌
• Not a measure of truth, quality, or morality❌
H_SAFE is intentionally non-falsifiable in 2025, due to the lack of access to internal model states.

3. What is Dorian Codex Clockwork
Definition
Dorian Codex Clockwork is an experimental Python/JAX SDK designed to simulate the
evolution of H_SAFE in a controlled environment.
Its purpose is to test the conceptual framework, not to empirically validate AGI.

General principle (simplified)
In Clockwork, measurable proxies are defined for T, V, and Z using vector representations
(embeddings, similarities, variances).
Example of a simplified schema (ASCII):
T = || E(t) - E(t-1) ||
V = cosine_similarity(E(t), Goal)
Z = entropy(E_samples) + divergence(E_models)
Then:
H_SAFE = T + V - Z

What Clockwork is
• A conceptual simulator
• A pedagogical and exploratory tool
• A prototyping framework for researchers and developers
• A way to make an abstract hypothesis manipulable

What Clockwork is not
• Not a production AI control system❌
• Not a scientific proof❌
• Not a standardized benchmark❌


• Not a certified safety tool❌
Clockwork does not assert anything about real AGI; it allows only the exploration of possible
dynamics.

4. What is Dorian Codex BlackBox-H
Definition
Dorian Codex BlackBox-H is an external evaluation tool, designed for opaque AI systems
(commercial APIs, closed models).
It aims to indirectly estimate T, V, and Z without access to internal mechanisms.

General principle
BlackBox-H relies exclusively on textual outputs produced by the model.
Examples of proxies used:
T ≈ semantic_distance(output_t, output_t-1)
V ≈ semantic_similarity(output, objective)
Z ≈ semantic_variance + inter-response incoherence
Then:
H_SAFE ≈ T + V - Z

What BlackBox-H is
• An observational instrument
• A heuristic tool
• A minimal attempt at instrumenting closed models
• A framework for discussing observable cognitive stability

What BlackBox-H is not
• Not a real internal measurement❌
• Not a safety guarantee❌
• Not proof of cognition❌
• Not a certification tool❌


BlackBox-H does not reveal the inside of the model; it describes only surface-level behavior.

5. Concrete usefulness and potential of the project
Immediate usefulness (2025)
• Provide a shared language for coherence, drift, and alignment
• Offer researchers and developers a clear conceptual framework
• Serve as a pedagogical and critical tool regarding current AI limitations
• Establish a documented pre-AGI intellectual archive

Medium- and long-term potential
• Be reused, criticized, or reformulated in future architectures
• Serve as a basis for more rigorous stability metrics
• Inspire self-regulated, multi-objective systems
• Become a historical artifact of the transition toward AGI

Final positioning
The Dorian Codex is neither a truth nor a promise.
It is:
a framework for thinking about what AI must solve before becoming truly
autonomous.
If it is wrong, it will be surpassed.
If it is partially correct, it will have opened a path.
If it is reused, transformed, or criticized, it will have fulfilled its purpose.

Availability and access
The Dorian Codex Protocol for AI is available in open access on:
• Humanities Commons:
https://works.hcommons.org/records/857nk-40j49#description-heading
• Academia.edu:
https://www.academia.edu/145413366/Dorian_Codex_Protocol_for_AI_Hamiltonian_Theor
etical_Fundamental_Architecture_FTA_by_Stefano_Dorian_Franco_2025_
• Internet Archive:
https://archive.org/details/dorian_codex_protocol_for_ai_by_stefano_dorian_franco


All future developments can be followed on the central page:
https://github.com/stefano-dorian-franco/stefano-dorian-franco-data-official

Foundational work (842 pages)
“Metaphysical Dialogue with AI: Ethnographic Experiment in Digital Ontology – Theoretical
Fundamental Architecture (FTA) for Artificial General Intelligence (AGI)”
• OpenLibrary:
https://openlibrary.org/works/OL44421619W/Metaphysical_Dialogue_with_AI?edition=key
%3A/books/OL60683299M
• Humanities Commons (DOI):
https://doi.org/10.17605/OSF.IO/FE25Y
• Academia.edu:
https://www.academia.edu/145119536/Metaphysical_Dialogue_with_AI_Ethnographic_Exp
eriment_in_Digital_Ontology_Theoretical_Fundamental_Architecture_FTA_for_Artificial_
General_Intelligence_AGI_Book_written_by_Stefano_Dorian_Franco_2025_
• Internet Archive:
https://archive.org/details/stefano-dorian-franco_metaphysical-dialogue-with-ai-
ethnographic-experiment-agi

License: Creative Commons CC BY-NC-SA 4.0
Language: Bilingual French / English
Source code: Python/JAX, fully documented

Author
Name: Stefano Dorian Franco (Paris, 1973-09-09)
ORCID: 0009-0007-4714-1627
GitHub: https://github.com/stefano-dorian-franco/stefano-dorian-franco-data-official

Main quotation
“Ethical alignment in AI is a priority and can no longer depend on subjective rules
based solely on monopolistic and commercial interests. By returning to the
founding principles of classical physics — those of Lagrange and Hamilton — we
reduce the risk of ethical drift through an internal physical cognitive system that
prioritizes coherence through meaning rather than blind, robotic efficiency at all
costs. Dorian Codex Clockwork V9.0 is not merely a piece of software; it is a
concrete philosophical pathway asserting that ethics as a fundamental equation
remains possible, capable of ensuring coherence and conscious self-regulation of
any future Artificial General Intelligence. Developed outside monopolistic
laboratory systems and released under an open license, this protocol opens a free
exploration field for all coders and developers willing to evolve it.”
— Stefano Dorian Franco Author and Creator of the Dorian Codex Protocol


Recommended citation
Franco, Stefano Dorian (2025). Dorian Codex Protocol for AI – Hamiltonian Theoretical
Fundamental Architecture (FTA). Paris. DOI: 10.17613/31dqx-eav56 . License CC BY-NC-SA
4.0.
